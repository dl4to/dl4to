# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/models/5_equivariance.ipynb (unless otherwise specified).

__all__ = ['ConvolutionalBlock', 'EncodingBlock', 'Encoder', 'DecodingBlock', 'Decoder', 'UNet', 'UNet3D',
           'DeepImagePrior', 'EquivarianceWrapper', 'EquivariantModel']

# Internal Cell
import torch
import torch.nn as nn

# Cell
class ConvolutionalBlock(nn.Module):
    """
    This class defines a convolutional block that can be used for the construction of convolutional neural networks (CNNs).
    """
    def __init__(
        self,
        dimensions:int, # The number of dimensions to consider. Possible options are 2 and 3.
        in_channels:int, # The number of input channels.
        out_channels:int, # The number of output channels.
        normalization:str=None, # The type of normalization to use. Possible options include "batch", "layer" and "instance".
        kernel_size:int=3, # The size of the convolutional kernel.
        activation:str='ReLU', # The activation function that should be used.
        preactivation:bool=False, # Whether to use preactivations.
        use_padding:bool=True, # Whether to use padding.
        padding_mode:str='zeros', # The type of padding to use.
        dilation:bool=None, # The amount of dilation that should be used.
        dropout:float=0, # The dropout rate.
    ):
        super().__init__()
        self.dimensions = dimensions
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.normalization = normalization
        self.kernel_size = kernel_size
        self.activation = activation
        self.preactivation = preactivation
        self.use_padding = use_padding
        self.padding_mode = padding_mode
        self.dropout = dropout

        if dilation is None:
            self.dilation = 1

        self._set_padding()

        self._set_conv_layer()
        self._set_norm_layer()
        self._set_activation_layer()
        self._set_dropout_layer()

        self.block = self._create_sequential_block_from_layers()


    def forward(self,
                x:torch.Tensor # The input to the convolutional block.
               ):
        """
        The forward pass of the convolutional block. Returns a `torch.Tensor`.
        """
        return self.block(x)


    def _set_padding(self):
        if self.use_padding:
            total_padding = self.dilation * (self.kernel_size - 1)
            self.padding = total_padding // 2
        else:
            self.padding = 0


    def _set_conv_layer(self):
        conv_class = getattr(nn, f'Conv{self.dimensions}d')
        no_bias = not self.preactivation and (self.normalization is not None)

        self.conv_layer = conv_class(
            in_channels=self.in_channels,
            out_channels=self.out_channels,
            kernel_size=self.kernel_size,
            padding=self.padding,
            padding_mode=self.padding_mode,
            dilation=self.dilation,
            bias=not no_bias
        )


    def _set_norm_layer(self):
        if self.normalization is None:
            self.norm_layer = None
        else:
            num_features = self.in_channels if self.preactivation else self.out_channels
            norm_class = getattr(nn, f'{self.normalization.capitalize()}Norm{self.dimensions}d')
            self.norm_layer = norm_class(num_features)


    def _set_activation_layer(self):
        if self.activation is None:
            self.activation_layer = None
        elif type(self.activation) == str:
            self.activation_layer = getattr(nn, self.activation)()
        else:
            self.activation_layer = self.activation


    def _set_dropout_layer(self):
        if self.dropout == 0 or self.dropout is None:
            self.dropout_layer = None
        else:
            dropout_class = getattr(nn, f'Dropout{self.dimensions}d')
            self.dropout_layer = dropout_class(p=self.dropout)


    def _create_sequential_block_from_layers(self):
        block = nn.ModuleList()

        if self.preactivation:
            self._add_if_not_none(block, self.norm_layer)
            self._add_if_not_none(block, self.activation_layer)
            self._add_if_not_none(block, self.conv_layer)
        else:
            self._add_if_not_none(block, self.conv_layer)
            self._add_if_not_none(block, self.norm_layer)
            self._add_if_not_none(block, self.activation_layer)

        self._add_if_not_none(block, self.dropout_layer)
        return nn.Sequential(*block)


    @staticmethod
    def _add_if_not_none(module_list, module):
        if module is not None:
            module_list.append(module)

# Internal Cell
import torch
import torch.nn as nn

from .models import ConvolutionalBlock

# Cell
class EncodingBlock(nn.Module):
    """
    This class defines a single encoding block for an encoder.
    """
    def __init__(
        self,
        in_channels:int, # The number of input channels.
        out_channels_first:int, # The number of output channels after the first encoding step.
        dimensions:int, # The number of dimensions to consider. Possible options are 2 and 3.
        normalization:str, # The type of normalization to use. Possible options include "batch", "layer" and "instance".
        pooling_type:str, # The type of pooling to use.
        pooling_kernel_size:int, # The size of the pooling kernel.
        preactivation:bool=False, # Whether to use preactivations.
        is_first_block:bool=False, # Whether this is the first block of an encoder.
        residual:bool=False, # Whether the encoder should be a residual network.
        use_padding:bool=False, # Whether to use padding.
        padding_mode:str='zeros', # The type of padding to use.
        activation:str='ReLU', # The activation function that should be used.
        dilation:int=None, # The amount of dilation that should be used.
        dropout:float=0., # The dropout rate.
    ):
        super().__init__()

        self.preactivation = preactivation
        self.normalization = normalization

        self.residual = residual

        if is_first_block:
            normalization = None
            preactivation = None
        else:
            normalization = self.normalization
            preactivation = self.preactivation

        self.conv1 = ConvolutionalBlock(
            dimensions=dimensions,
            in_channels=in_channels,
            out_channels=out_channels_first,
            normalization=normalization,
            preactivation=preactivation,
            use_padding=use_padding,
            padding_mode=padding_mode,
            activation=activation,
            dilation=dilation,
            dropout=dropout
        )

        if dimensions == 2:
            out_channels_second = out_channels_first
        elif dimensions == 3:
            out_channels_second = 2 * out_channels_first

        self.conv2 = ConvolutionalBlock(
            dimensions=dimensions,
            in_channels=out_channels_first,
            out_channels=out_channels_second,
            normalization=self.normalization,
            preactivation=self.preactivation,
            use_padding=use_padding,
            activation=activation,
            dilation=dilation,
            dropout=dropout
        )

        if residual:
            self.conv_residual = ConvolutionalBlock(
                dimensions=dimensions,
                in_channels=in_channels,
                out_channels=out_channels_second,
                kernel_size=1,
                normalization=None,
                activation=None
            )

        self._set_downsampling_layer(dimensions, pooling_type, kernel_size=pooling_kernel_size)


    def forward(self,
                x:torch.Tensor # the input to the encoding block.
               ):
        """
        The forward pass of the encoding block.
        Returns a list of `torch.Tensors` that define the outputs of the skip connections, and a `torch.Tensor` that is the output of the encoding block.
        """
        if self.residual:
            connection = self.conv_residual(x)
            x = self.conv1(x)
            x = self.conv2(x)
            x += connection
        else:
            x = self.conv1(x)
            x = self.conv2(x)

        if self.downsample is None:
            return x

        skip_connection = x
        x = self.downsample(x)
        return x, skip_connection


    @property
    def out_channels(self):
        return self.conv2.conv_layer.out_channels


    def _set_downsampling_layer(self, dimensions, pooling_type, kernel_size, stride=2):
        if pooling_type is None:
            self.downsample = None
        else:
            class_name = '{}Pool{}d'.format(pooling_type.capitalize(), dimensions)
            class_ = getattr(nn, class_name)
            self.downsample = class_(kernel_size, stride)

# Cell
class Encoder(nn.Module):
    """
    This class defines an encoder that can be used for the construction of UNets [1].
    An encoder is a neural network that takes the input, and outputs a feature vector for each input sample.
    """
    def __init__(
        self,
        in_channels:int, # The number of input channels.
        out_channels_first:int, # The number of output channels after the first encoding step.
        dimensions:int, # The number of dimensions to consider. Possible options are 2 and 3.
        pooling_type:str, # The type of pooling to use.
        num_encoding_blocks:int, # The number of encoding blocks.
        normalization:str, # The type of normalization to use. Possible options include "batch", "layer" and "instance".
        pooling_kernel_size:int, # The size of the pooling kernel.
        preactivation:bool=False, # Whether to use preactivations.
        residual:bool=False, # Whether the encoder should be a residual network.
        use_padding:bool=False, # Whether to use padding.
        padding_mode:str='zeros', # The type of padding to use.
        activation:str='ReLU', # The activation function that should be used.
        initial_dilation:int=None, # The amount of dilation that should be used in the first encoding block.
        dropout:float=0. # The dropout rate.
    ):
        super().__init__()

        if len(pooling_kernel_size) != dimensions:
            raise ValueError('Length of pooling_kernel_size and dimension do not coincide!')

        self.encoding_blocks = nn.ModuleList()
        self.dilation = initial_dilation
        is_first_block = True

        for _ in range(num_encoding_blocks):
            encoding_block = EncodingBlock(
                in_channels=in_channels,
                out_channels_first=out_channels_first,
                dimensions=dimensions,
                normalization=normalization,
                pooling_type=pooling_type,
                preactivation=preactivation,
                is_first_block=is_first_block,
                residual=residual,
                use_padding=use_padding,
                padding_mode=padding_mode,
                activation=activation,
                dilation=self.dilation,
                dropout=dropout,
                pooling_kernel_size=pooling_kernel_size
            )

            is_first_block = False
            self.encoding_blocks.append(encoding_block)

            if dimensions == 2:
                in_channels = out_channels_first
                out_channels_first = in_channels * 2
            elif dimensions == 3:
                in_channels = 2 * out_channels_first
                out_channels_first = in_channels

            if self.dilation is not None:
                self.dilation *= 2


    @property
    def out_channels(self):
        return self.encoding_blocks[-1].out_channels


    def forward(self,
                x:torch.Tensor # The input of the encoder.
               ):
        """
        The forward pass of the encoder.
        Returns a list of `torch.Tensors` that define the outputs of the skip connections, and a `torch.Tensor` that is the output of the encoder.
        """
        skip_connections = []

        for encoding_block in self.encoding_blocks:
            x, skip_connection = encoding_block(x)
            skip_connections.append(skip_connection)

        return skip_connections, x

# Internal Cell
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

from .models import ConvolutionalBlock

# Cell
class DecodingBlock(nn.Module):
    """
    This class defines a decoding block for the decoder.
    """
    def __init__(
        self,
        in_channels_skip_connection:int, # The number of input channels from the skip connections of the encoder.
        dimensions:int, # The number of dimensions to consider. Possible options are 2 and 3.
        upsampling_type:str, # The type of upsampling to use.
        normalization:str, # The type of normalization to use. Possible options include "batch", "layer" and "instance".
        preactivation:bool, # Whether to use preactivations.
        residual:bool=False, # Whether the decoder should be a residual network.
        use_padding:bool=False, # Whether to use padding.
        padding_mode:str='zeros', # The type of padding to use.
        activation:str='ReLU', # The activation function that should be used.
        dilation:int=None, # The amount of dilation that should be used.
        dropout:float=0., # The dropout rate.
        upsample_recover_orig_size:bool=False, # Whether the original input size of the encoder should be recovered with the decoder output.
    ):
        super().__init__()

        self.residual = residual
        self.upsampling_type = upsampling_type
        self.upsample_recover_orig_size = upsample_recover_orig_size

        if upsampling_type == 'conv':
            if upsample_recover_orig_size:
                print("Ignoring upsample_recover_orig_size=False when using upsampling_type=conv.")

            in_channels = out_channels = 2 * in_channels_skip_connection
            self.upsample = self._get_conv_transpose_layer(dimensions, in_channels, out_channels)
        else:
            self.upsample = self._get_upsampling_layer(upsampling_type)

        in_channels_first = 3 * in_channels_skip_connection
        out_channels = in_channels_skip_connection

        self.conv1 = ConvolutionalBlock(
            dimensions=dimensions,
            in_channels=in_channels_first,
            out_channels=out_channels,
            normalization=normalization,
            preactivation=preactivation,
            use_padding=use_padding,
            padding_mode=padding_mode,
            activation=activation,
            dilation=dilation,
            dropout=dropout
        )

        in_channels_second = out_channels

        self.conv2 = ConvolutionalBlock(
            dimensions=dimensions,
            in_channels=in_channels_second,
            out_channels=out_channels,
            normalization=normalization,
            preactivation=preactivation,
            use_padding=use_padding,
            padding_mode=padding_mode,
            activation=activation,
            dilation=dilation,
            dropout=dropout
        )

        if residual:
            self.conv_residual = ConvolutionalBlock(
                dimensions=dimensions,
                in_channels=in_channels_first,
                out_channels=out_channels,
                kernel_size=1,
                normalization=None,
                activation=None
            )



    def forward(self,
                skip_connection:list, # A list of `torch.Tensors` that contain the outputs of the skip connections from an encoding block.
                x:torch.Tensor # The input to the decoding block.
               ):
        """
        The forward pass of the decoding block.
        """
        if self.upsampling_type != 'conv':
            if self.upsample_recover_orig_size:
                self.upsample.size = skip_connection.shape[-3:]
            else:
                self.upsample.scale_factor = 2.

        x = self.upsample(x)
        skip_connection = self._center_crop(skip_connection, x)
        x = torch.cat((skip_connection, x), dim=1)

        if self.residual:
            connection = self.conv_residual(x)
            x = self.conv1(x)
            x = self.conv2(x)
            x += connection
        else:
            x = self.conv1(x)
            x = self.conv2(x)

        return x


    def _get_upsampling_layer(self, upsampling_type):
        upsampling_modes = ('nearest', 'linear', 'bilinear', 'bicubic', 'trilinear')

        if upsampling_type not in upsampling_modes:
            message = (f'Upsampling type is {upsampling_type} but should be one of the following: {upsampling_modes}.')
            raise ValueError(message)

        upsample = nn.Upsample(mode=upsampling_type)
        return upsample


    def _get_conv_transpose_layer(self, dimensions, in_channels, out_channels):
        conv_class = getattr(nn, f'ConvTranspose{dimensions}d')
        conv_layer = conv_class(in_channels, out_channels, kernel_size=2, stride=2)
        return conv_layer


    def _center_crop(self, skip_connection, x):
        skip_shape = np.array(skip_connection.shape)
        x_shape = np.array(x.shape)

        crop = skip_shape[2:] - x_shape[2:]
        half_crop = torch.tensor(crop // 2)

        pad = -torch.stack((half_crop, half_crop)).t().flatten()

        skip_connection = F.pad(skip_connection, pad.tolist())
        return skip_connection

# Cell
class Decoder(nn.Module):
    """
    Defines a decoder that can be used for the construction of UNets [1].
    The decoder is a neural network that takes the feature vector from the encoder and decodes it into an output.
    """
    def __init__(
        self,
        in_channels_skip_connection:int, # The number of input channels from the skip connections of the encoder.
        dimensions:int, # The number of dimensions to consider. Possible options are 2 and 3.
        upsampling_type:str, # The type of upsampling to use.
        num_decoding_blocks:int, # The number of decoding blocks.
        normalization:str, # The type of normalization to use. Possible options include "batch", "layer" and "instance".
        preactivation:bool, # Whether to use preactivations.
        residual:bool=False, # Whether the decoder should be a residual network.
        use_padding:bool=False, # Whether to use padding.
        padding_mode:str='zeros', # The type of padding to use.
        activation:str='ReLU', # The activation function that should be used.
        initial_dilation:int=None, # The amount of dilation that should be used in the first encoding block.
        dropout:float=0., # The dropout rate.
        upsample_recover_orig_size:bool=False, # Whether the original input size of the encoder should be recovered with the decoder output.
    ):
        super().__init__()
        upsampling_type = self._fix_upsampling_type(upsampling_type, dimensions)

        self.decoding_blocks = nn.ModuleList()
        self.dilation = initial_dilation

        for _ in range(num_decoding_blocks):
            decoding_block = DecodingBlock(
                in_channels_skip_connection=in_channels_skip_connection,
                dimensions=dimensions,
                upsampling_type=upsampling_type,
                normalization=normalization,
                preactivation=preactivation,
                residual=residual,
                use_padding=use_padding,
                padding_mode=padding_mode,
                activation=activation,
                dilation=self.dilation,
                dropout=dropout,
                upsample_recover_orig_size=upsample_recover_orig_size
            )

            self.decoding_blocks.append(decoding_block)
            in_channels_skip_connection = in_channels_skip_connection // 2

            if self.dilation is not None:
                self.dilation = self.dilation // 2


    def _fix_upsampling_type(self, upsampling_type, dimensions):
        if upsampling_type == 'linear':
            if dimensions == 2:
                upsampling_type = 'bilinear'
            elif dimensions == 3:
                upsampling_type = 'trilinear'

        return upsampling_type


    def forward(self,
                skip_connections:list, # A list of `torch.Tensors` that contain the outputs of the skip connections from an encoder.
                x:torch.Tensor # The input to the decoder.
               ):
        """
        The forward pass of the decoder.
        """
        zipped = zip(reversed(skip_connections), self.decoding_blocks)

        for skip_connection, decoding_block in zipped:
            x = decoding_block(skip_connection, x)

        return x

# Internal Cell
import torch
import torch.nn as nn
from typing import Union

from .models import ConvolutionalBlock, Encoder, EncodingBlock, Decoder

# Cell
class UNet(nn.Module):
    """
    UNets are convolutional autoencoders that were developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg [1].
    The network is based on a fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations.
    Our code based on `https://github.com/fepegar/unet/tree/master/unet`.
    """
    def __init__(
        self,
        in_channels:int=7, # The number of input channels.
        out_classes:int=1, # The number of output classes.
        dimensions:int=3, # The number of dimensions to consider. Possible options are 2 and 3.
        num_encoding_blocks:int=4, # The number of encoding blocks.
        out_channels_first_layer:int=10, # The number of output channels after the first encoding step.
        normalization:str='batch', # The type of normalization to use. Possible options include "batch", "layer" and "instance".
        pooling_type:str='max', # The type of pooling to use.
        upsampling_type:str='nearest', # The type of upsampling to use.
        preactivation:bool=False, # Whether to use preactivations.
        residual:bool=False, # Whether the encoder should be a residual network.
        use_padding:bool=True, # Whether to use padding.
        padding_mode:str='zeros', # The type of padding to use.
        activation:str='ReLU', # The activation function that should be used.
        classifier_activation:str='Sigmoid', # The activation function for the classifier at the end of the UNet.
        initial_dilation:int=None, # The amount of dilation that should be used in the first encoding block.
        dropout:float=0., # The dropout rate.
        upsample_recover_orig_size:bool=True, # Whether the original input size of the encoder should be recovered with the decoder output.
        pooling_kernel_size:Union[int,list]=[3, 3, 3], # The size of the pooling kernel.
        use_classifier:bool=True, # Whether to use a classifier layer at the end of the network.
        verbose:bool=True # Whether to print the user information on the neural network, for instance the number of parameters.
    ):
        super().__init__()

        self.in_channels = in_channels
        self.out_classes = out_classes
        self.dimensions = dimensions
        self.num_encoding_blocks = num_encoding_blocks
        self.out_channels_first_layer = out_channels_first_layer
        self.normalization = normalization
        self.pooling_type = pooling_type
        self.upsampling_type = upsampling_type
        self.preactivation = preactivation
        self.residual = residual
        self.use_padding = use_padding
        self.padding_mode = padding_mode
        self.activation = activation
        self.classifier_activation = classifier_activation
        self.initial_dilation = initial_dilation
        self.dropout = dropout
        self.upsample_recover_orig_size = upsample_recover_orig_size
        self.pooling_kernel_size = pooling_kernel_size
        self.use_classifier = use_classifier
        self.verbose = verbose

        self.depth = self.num_encoding_blocks - 1

        if residual:
            self.use_padding = True

        if len(pooling_kernel_size) != dimensions:
            raise ValueError('Length of pooling_kernel_size and dimension do not coincide!')


        self.encoder = self._get_encoder()
        self.bottom_block = self._get_bottom_block()
        self.decoder = self._get_decoder()

        if self.use_classifier:
            self.classifier = self._get_classifier()


        if verbose:
            n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
            print(f'Built model with {n_params} parameters.')


    def _get_encoder(self):
        encoder = Encoder(
            in_channels=self.in_channels,
            out_channels_first=self.out_channels_first_layer,
            dimensions=self.dimensions,
            pooling_type=self.pooling_type,
            num_encoding_blocks=self.depth,
            normalization=self.normalization,
            preactivation=self.preactivation,
            residual=self.residual,
            use_padding=self.use_padding,
            padding_mode=self.padding_mode,
            activation=self.activation,
            initial_dilation=self.initial_dilation,
            dropout=self.dropout,
            pooling_kernel_size=self.pooling_kernel_size
        )
        return encoder


    def _get_bottom_block(self):
        out_channels_first = self.encoder.out_channels

        if self.dimensions == 2:
            out_channels_first = 2 * out_channels_first

        bottom_block = EncodingBlock(
            in_channels=self.encoder.out_channels,
            out_channels_first=out_channels_first,
            dimensions=self.dimensions,
            normalization=self.normalization,
            pooling_type=None,
            preactivation=self.preactivation,
            residual=self.residual,
            use_padding=self.use_padding,
            padding_mode=self.padding_mode,
            activation=self.activation,
            dilation=self.encoder.dilation,
            dropout=self.dropout,
            pooling_kernel_size=self.pooling_kernel_size
        )
        return bottom_block


    def _get_decoder(self):
        power = self.depth

        if self.dimensions == 2:
            power = power - 1

        decoder = Decoder(
            in_channels_skip_connection=self.out_channels_first_layer * 2**power,
            dimensions=self.dimensions,
            upsampling_type=self.upsampling_type,
            num_decoding_blocks=self.depth,
            normalization=self.normalization,
            preactivation=self.preactivation,
            residual=self.residual,
            use_padding=self.use_padding,
            padding_mode=self.padding_mode,
            activation=self.activation,
            initial_dilation=self.encoder.dilation,
            dropout=self.dropout,
            upsample_recover_orig_size=self.upsample_recover_orig_size
        )
        return decoder


    def _get_classifier(self):
        in_channels = self.bottom_block.out_channels

        if self.dimensions == 2:
            in_channels = self.out_channels_first_layer
        elif self.dimensions == 3:
            in_channels = 2 * self.out_channels_first_layer

        classifier = ConvolutionalBlock(
            dimensions=self.dimensions,
            in_channels=in_channels,
            out_channels=self.out_classes,
            kernel_size=1,
            activation=self.classifier_activation
        )
        return classifier


    def forward(self,
                model_inputs:torch.Tensor # The input to the UNet.
               ):
        """
        The forward pass of the UNet. Returns a `torch.Tensor` object.
        """
        skip_connections, encoding = self.encoder(model_inputs)
        encoding = self.bottom_block(encoding)

        output = self.decoder(skip_connections, encoding)

        if self.use_classifier:
            return self.classifier(output)
        return output

# Cell
class UNet3D(UNet):
    """
    A 3d version of our UNet. UNets are convolutional autoencoders that were developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg [1].
    The network is based on a fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations.
    Our code based on `https://github.com/fepegar/unet/tree/master/unet`.
    """
    def __init__(self, *args, **user_kwargs):
        kwargs = {}
        kwargs['dimensions'] = 3
        kwargs.update(user_kwargs)
        super().__init__(*args, **kwargs)

# Internal Cell
import torch
import torch.nn as nn

# Cell
class DeepImagePrior(nn.Module):
    """
    The deep image prior (DIP) [1] is a type of convolutional neural network used to enhance a given image with no prior training data other than the image itself.
    A neural network is randomly initialized and used as prior to solve inverse problems such as noise reduction, super-resolution, and inpainting.
    Image statistics are captured by the structure of a convolutional image generator rather than by any previously learned capabilities.
    """
    def __init__(self,
                 shape:list, # A list containing three entries that define the number of voxels in each direction.
                 n_channels:int=1, # The number of input channels.
                 n_inital_channels:int=4 #T he number of channels after the first encoding block. The model has a total 4 encoding and 4 decoding blocks, and the number of channels is doubled in each encoding step.
                ):
        super().__init__()
        self.encoder_layers = nn.ModuleList()
        self.n_channels = n_channels

        self.encoder_layers.append(nn.Conv3d(n_channels,    1*n_inital_channels, 3, stride=2, padding=1))
        self.encoder_layers.append(nn.Conv3d(1*n_inital_channels, 2*n_inital_channels, 3, stride=2, padding=1))
        self.encoder_layers.append(nn.Conv3d(2*n_inital_channels, 4*n_inital_channels, 3, stride=2, padding=1))
        self.encoder_layers.append(nn.Conv3d(4*n_inital_channels, 8*n_inital_channels, 3, stride=2, padding=1))

        self.decoder_layers = nn.ModuleList()
        self.decoder_layers.append(nn.Conv3d(8*n_inital_channels, 4*n_inital_channels, 3, stride=1, padding=1))
        self.decoder_layers.append(nn.Conv3d(4*n_inital_channels, 2*n_inital_channels, 3, stride=1, padding=1))
        self.decoder_layers.append(nn.Conv3d(2*n_inital_channels, 1*n_inital_channels, 3, stride=1, padding=1))
        self.decoder_layers.append(nn.Conv3d(1*n_inital_channels,    n_channels, 3, stride=1, padding=1))

        self.relu = nn.ReLU()
        self.z = torch.randn(n_channels, *shape, requires_grad=False)


    def forward(self):
        """
        The forward pass of the DIP with a fixed random noise input. Returns a `torch.Tensor` object.
        """
        encoder_activations = [self.z.view(1, *self.z.shape)]

        for encoder_layer in self.encoder_layers[:-1]:
            activation = encoder_activations[-1]
            activation = nn.functional.layer_norm(activation, encoder_activations[-1].shape[1:])
            activation = encoder_layer(activation)
            activation = self.relu(activation)
            encoder_activations.append(activation)

        central_activation = self.relu(self.encoder_layers[-1](encoder_activations[-1]))

        decoder_activations = [central_activation]

        for idx, (decoder_layer, encoder_activation) in enumerate(zip(self.decoder_layers, encoder_activations[::-1])):
            activation = decoder_activations[-1]
            activation = nn.functional.layer_norm(activation, activation.shape[1:])
            activation = nn.functional.interpolate(activation, size=encoder_activation.shape[2:])
            activation = decoder_layer(activation)
            activation = self.relu(activation) if idx != len(self.decoder_layers) - 1 else torch.sigmoid(activation)
            decoder_activations.append(activation)

        return decoder_activations[-1].squeeze(0)

# Internal Cell
import torch
import random
from itertools import product

# Cell
class EquivarianceWrapper:
    """
    A class that represents an equivariance wrapper [1] that implements group equivariance via group averaging [2].
    """
    def __init__(self,
                 preprocessing:"dl4to.preprocessing.Preprocessing"=None, # The preprocessing strategy to use. This is used in the equivariance wrapper to obtain the scalar and vector field information of the input.
                 rotate:bool=True, # Whether to include rotational equivariance in the transformation group.
                 mirror:bool=True, # Whether to include mirror equivariance in the transformation group.
                 dim:int=2, # The dimension of the transformation group. Specifically, a 2d transformation group does not consider rotations and mirrors along the z-axis.
                 rotate_twice:bool=False, # Whether double-rotations should be used, where the input is rotated twice, along two different axes. This may result in a larger transformation group.
                 sample_rate:float=1. # The rate of transformations that should be randomly sampled in the forward pass. `sample_rate=1.` defaults to all transformations being used in the wrapper. A smaller choice may be beneficial if memory constraints don't allow for the applications of all transformations in each forward pass.
                ):
        self.preprocessing = preprocessing
        assert dim == 2 or dim == 3
        self.rotate = rotate
        self.mirror = mirror
        self.dim = dim
        self.sample_rate = sample_rate
        self.cube_face_indices = self._get_cube_face_indices()
        self.rotate_twice = rotate_twice
        self.name = self._get_name()


    @property
    def preprocessing(self):
        return self._preprocessing


    @preprocessing.setter
    def preprocessing(self, preprocessing):
        self._preprocessing = preprocessing
        if preprocessing is not None:
            vector_directions = preprocessing.vector_directions
            self.vector_field_channels = self._get_vector_field_channels(vector_directions)



    def _get_name(self):
        name = "equiv"
        if self.rotate:
            name += "_rot"
        if self.mirror:
            name += "_mir"
        if self.rotate_twice:
            name += "_twice"
        name += f"_{self.dim}d"
        return name


    def _get_cube_face_indices(self):
        cube_face_indices = [0, 1, 3, 4]
        if self.dim == 3:
            cube_face_indices.extend([2, 5])
        return cube_face_indices


    def _get_vector_field_channels(self, vector_directions):
        x_channel_idx = [i for i, vector_direction in enumerate(vector_directions) if vector_direction == 'x']
        y_channel_idx = [i for i, vector_direction in enumerate(vector_directions) if vector_direction == 'y']
        z_channel_idx = [i for i, vector_direction in enumerate(vector_directions) if vector_direction == 'z']
        return [x_channel_idx, y_channel_idx, z_channel_idx]


    def mirror_input(self,
                     x:torch.Tensor, # The input that should be mirrored/flipped.
                     flip_dimensions:list # The dimension along which the input should be mirrored.
                    ):
        """
        Returnes a `torch.Tensor`, which is a mirrored version of the input `x`.
        """
        x = torch.flip(x, flip_dimensions)
        x = self._mirror_vector_fields(x, flip_dimensions)
        return x


    def _mirror_vector_fields(self, x, flip_dimensions):
        for flip_dimension in flip_dimensions:
            for flip_channel in self.vector_field_channels[flip_dimension]:
                x[:,flip_channel] = -x[:,flip_channel]
        return x


    def rotate_input(self,
                     x:torch.Tensor, # The input that should be rotated.
                     rotations:int, # The number of 90Â° rations that should be performed. Four rotations result in the identity.
                     plane:list # On which plane the input should be rotated.
                    ):
        """
        Returnes a `torch.Tensor`, which is a rotated version of the input `x`.
        """
        x = torch.rot90(x, rotations, plane)
        x = self._rotate_vector_fields(x, rotations, plane)
        return x


    def _rotate_vector_fields(self, x, rotations, plane):
        assert plane[0] < 0 and plane[1] < 0, "Indices of rotational plane must be given in negative integers"
        assert len(x.shape) == 5
        force_index_0, force_index_1 = self.vector_field_channels[plane[0]], self.vector_field_channels[plane[1]]

        if rotations % 4 == 1:
            temp = x[:,force_index_0].clone()
            x[:,force_index_0] = -x[:,force_index_1]
            x[:,force_index_1] = temp

        if rotations % 4 == 2:
            x[:,force_index_0] = -x[:,force_index_0]
            x[:,force_index_1] = -x[:,force_index_1]

        if rotations % 4 == 3:
            temp = x[:,force_index_0].clone()
            x[:,force_index_0] = x[:,force_index_1]
            x[:,force_index_1] = -temp
        return x


    def __get_rotations_and_plane_for_first_rotation(self, cube_face_index):
        if cube_face_index == 0: rotations, plane =  0, [-3, -2]
        if cube_face_index == 1: rotations, plane =  1, [-3, -2]
        if cube_face_index == 2: rotations, plane = -1, [-1, -3]
        if cube_face_index == 3: rotations, plane =  2, [-3, -2]
        if cube_face_index == 4: rotations, plane =  3, [-3, -2]
        if cube_face_index == 5: rotations, plane =  1, [-1, -3]
        return rotations, plane


    def __get_rotations_and_plane_for_second_rotation(self, cube_face_index):
        rotations = [0,2]
        if cube_face_index%3 == 0: plane = [-2, -1]
        if cube_face_index%3 == 1: plane = [-1, -3]
        if self.dim == 3:
            rotations.extend([1, 3])
            if cube_face_index%3 == 2: plane = [-3, -2]
        return rotations, plane


    def _get_rotations(self):
        transforms = []
        for cube_face_index in self.cube_face_indices:
            rotations, plane = self.__get_rotations_and_plane_for_first_rotation(cube_face_index)
            face_transform = lambda x, rotations=rotations, plane=plane: self.rotate_input(x, rotations, plane)
            inverse_face_transform = lambda x, rotations=rotations, plane=plane: torch.rot90(x, -rotations, plane)

            if self.rotate_twice:
                rotations, plane = self.__get_rotations_and_plane_for_second_rotation(cube_face_index)
                for rotation in rotations:
                    transform = lambda x, face_transform=face_transform, rotation=rotation, plane=plane: self.rotate_input(face_transform(x), rotation, plane)
                    inverse_transform = lambda x, inverse_face_transform=inverse_face_transform, rotation=rotation, plane=plane: inverse_face_transform(torch.rot90(x, -rotation, plane))
                    transforms.append((transform, inverse_transform))
            else:
                transforms.append((face_transform, inverse_face_transform))
        return transforms


    def _get_flip_dimensions(self, rotation_transforms):
        flip_dimensions = []

        if len(rotation_transforms) != 1:
            bool_combinations = [[True]]
        else:
            bool_combinations = [[True, False] for _ in range(self.dim)]

        for axes in product(*bool_combinations):
            flip_dimension = []

            for i, axis in enumerate(axes):
                if axis:
                    flip_dimension.append(-3+i)

            flip_dimensions.append(flip_dimension)
        return flip_dimensions


    def _get_mirrors(self, rotation_transforms):
        transforms = []
        flip_dimensions = self._get_flip_dimensions(rotation_transforms)

        for rotation_transform, inverse_rotation_transform in rotation_transforms:
            for flip_dimension in flip_dimensions:
                transform = lambda x, rotation_transform=rotation_transform, flip_dimension=flip_dimension: rotation_transform(self.mirror_input(x, flip_dimension))
                inverse_transform = lambda x, inverse_rotation_transform=inverse_rotation_transform, flip_dimension=flip_dimension: torch.flip(inverse_rotation_transform(x), flip_dimension)
                transforms.append((transform, inverse_transform))
        return transforms


    def _sample_transforms(self, transforms, sample_rate):
        if sample_rate == 1.:
            return transforms
        number_of_sampled_transforms = max(1, round(len(transforms) * sample_rate))
        sampled_transforms = random.sample(transforms, k=number_of_sampled_transforms)
        return sampled_transforms


    def get_transforms(self,
                       sample_rate:float=None # The rate of transformations that should be randomly samples from the equivariance wrapper. `None` defaults to `equivariance_wrapper.sample_rate`. `1.` means that all transformations are considered.
                      ):
        """
        Returns a list of all group actions that are applied to an input in the equivariance wrapper.
        """
        if sample_rate is None:
            sample_rate = self.sample_rate
        rotation_transforms = [(lambda x: x, lambda x: x)]
        transforms = []
        if self.rotate:
            rotation_transforms = self._get_rotations()
            transforms = rotation_transforms
        if self.mirror:
            transforms += self._get_mirrors(rotation_transforms)
        transforms = self._sample_transforms(transforms, sample_rate)
        return transforms


    def __call__(self,
                 model:torch.nn.Module # The model that should be turned into an equivariant model.
                ):
        """
        Applies the equivariance wrapper to a `torch.nn.Module` model object and returns an `dl4to.models.EquivariantModel` object.
        """
        assert self.vector_field_channels is not None, print("EquivarianceWrapper does not have a preprocessing.")
        return EquivariantModel(model=model, equivariance_wrapper=self)

# Cell
class EquivariantModel(torch.nn.Module):
    """
    A class that represents an equivariant model with respect to a specific equivariance wrapper.
    """
    def __init__(self,
                 model:torch.nn.Module, # A PyTorch neural network.
                 equivariance_wrapper:"dl4to.models.EquivarianceWrapper" # The equivariance wrapper that is applied to the model.
                ):
        super().__init__()
        self.model = model
        self.equivariance_wrapper = equivariance_wrapper


    def __call__(self,
                 model_inputs:torch.Tensor, # The model inputs that are obtained as output of the preprocessing.
                 sample_rate:float=None # The rate of transformations that should be randomly samples from the equivariance wrapper. `None` defaults to `equivariance_wrapper.sample_rate`.`1.` means that all transformations are applied in the forward pass.
                ):
        """
        The forward method for the equivariant model.
        """
        assert model_inputs.shape[1] == len(self.equivariance_wrapper.preprocessing.vector_directions)
        transforms = self.equivariance_wrapper.get_transforms(sample_rate)
        model_outputs = 0
        for transform, inverse_transform in transforms:
            model_inputs_transformed = transform(model_inputs)
            model_outputs_transformed = self.model(model_inputs_transformed)
            model_outputs += inverse_transform(model_outputs_transformed)
        return model_outputs / len(transforms)